{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yKgiYEUdVSXp",
        "outputId": "91e9eae2-5565-47d9-f6ae-700ff513e090"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting pymupdf\n",
            "  Downloading pymupdf-1.26.3-cp39-abi3-manylinux_2_28_x86_64.whl.metadata (3.4 kB)\n",
            "Requirement already satisfied: langchain in /usr/local/lib/python3.11/dist-packages (0.3.26)\n",
            "Collecting faiss-cpu\n",
            "  Downloading faiss_cpu-1.11.0-cp311-cp311-manylinux_2_28_x86_64.whl.metadata (4.8 kB)\n",
            "Requirement already satisfied: openai in /usr/local/lib/python3.11/dist-packages (1.93.0)\n",
            "Requirement already satisfied: tiktoken in /usr/local/lib/python3.11/dist-packages (0.9.0)\n",
            "Collecting langchain-community\n",
            "  Downloading langchain_community-0.3.27-py3-none-any.whl.metadata (2.9 kB)\n",
            "Requirement already satisfied: langchain-core<1.0.0,>=0.3.66 in /usr/local/lib/python3.11/dist-packages (from langchain) (0.3.68)\n",
            "Requirement already satisfied: langchain-text-splitters<1.0.0,>=0.3.8 in /usr/local/lib/python3.11/dist-packages (from langchain) (0.3.8)\n",
            "Requirement already satisfied: langsmith>=0.1.17 in /usr/local/lib/python3.11/dist-packages (from langchain) (0.4.4)\n",
            "Requirement already satisfied: pydantic<3.0.0,>=2.7.4 in /usr/local/lib/python3.11/dist-packages (from langchain) (2.11.7)\n",
            "Requirement already satisfied: SQLAlchemy<3,>=1.4 in /usr/local/lib/python3.11/dist-packages (from langchain) (2.0.41)\n",
            "Requirement already satisfied: requests<3,>=2 in /usr/local/lib/python3.11/dist-packages (from langchain) (2.32.3)\n",
            "Requirement already satisfied: PyYAML>=5.3 in /usr/local/lib/python3.11/dist-packages (from langchain) (6.0.2)\n",
            "Requirement already satisfied: numpy<3.0,>=1.25.0 in /usr/local/lib/python3.11/dist-packages (from faiss-cpu) (2.0.2)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from faiss-cpu) (24.2)\n",
            "Requirement already satisfied: anyio<5,>=3.5.0 in /usr/local/lib/python3.11/dist-packages (from openai) (4.9.0)\n",
            "Requirement already satisfied: distro<2,>=1.7.0 in /usr/local/lib/python3.11/dist-packages (from openai) (1.9.0)\n",
            "Requirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from openai) (0.28.1)\n",
            "Requirement already satisfied: jiter<1,>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from openai) (0.10.0)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.11/dist-packages (from openai) (1.3.1)\n",
            "Requirement already satisfied: tqdm>4 in /usr/local/lib/python3.11/dist-packages (from openai) (4.67.1)\n",
            "Requirement already satisfied: typing-extensions<5,>=4.11 in /usr/local/lib/python3.11/dist-packages (from openai) (4.14.1)\n",
            "Requirement already satisfied: regex>=2022.1.18 in /usr/local/lib/python3.11/dist-packages (from tiktoken) (2024.11.6)\n",
            "Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in /usr/local/lib/python3.11/dist-packages (from langchain-community) (3.11.15)\n",
            "Requirement already satisfied: tenacity!=8.4.0,<10,>=8.1.0 in /usr/local/lib/python3.11/dist-packages (from langchain-community) (8.5.0)\n",
            "Collecting dataclasses-json<0.7,>=0.5.7 (from langchain-community)\n",
            "  Downloading dataclasses_json-0.6.7-py3-none-any.whl.metadata (25 kB)\n",
            "Collecting pydantic-settings<3.0.0,>=2.4.0 (from langchain-community)\n",
            "  Downloading pydantic_settings-2.10.1-py3-none-any.whl.metadata (3.4 kB)\n",
            "Collecting httpx-sse<1.0.0,>=0.4.0 (from langchain-community)\n",
            "  Downloading httpx_sse-0.4.1-py3-none-any.whl.metadata (9.4 kB)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (2.6.1)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (1.4.0)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (25.3.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (1.7.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (6.6.3)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (0.3.2)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (1.20.1)\n",
            "Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.11/dist-packages (from anyio<5,>=3.5.0->openai) (3.10)\n",
            "Collecting marshmallow<4.0.0,>=3.18.0 (from dataclasses-json<0.7,>=0.5.7->langchain-community)\n",
            "  Downloading marshmallow-3.26.1-py3-none-any.whl.metadata (7.3 kB)\n",
            "Collecting typing-inspect<1,>=0.4.0 (from dataclasses-json<0.7,>=0.5.7->langchain-community)\n",
            "  Downloading typing_inspect-0.9.0-py3-none-any.whl.metadata (1.5 kB)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.11/dist-packages (from httpx<1,>=0.23.0->openai) (2025.6.15)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.11/dist-packages (from httpx<1,>=0.23.0->openai) (1.0.9)\n",
            "Requirement already satisfied: h11>=0.16 in /usr/local/lib/python3.11/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->openai) (0.16.0)\n",
            "Requirement already satisfied: jsonpatch<2.0,>=1.33 in /usr/local/lib/python3.11/dist-packages (from langchain-core<1.0.0,>=0.3.66->langchain) (1.33)\n",
            "Requirement already satisfied: orjson<4.0.0,>=3.9.14 in /usr/local/lib/python3.11/dist-packages (from langsmith>=0.1.17->langchain) (3.10.18)\n",
            "Requirement already satisfied: requests-toolbelt<2.0.0,>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from langsmith>=0.1.17->langchain) (1.0.0)\n",
            "Requirement already satisfied: zstandard<0.24.0,>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from langsmith>=0.1.17->langchain) (0.23.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.33.2 in /usr/local/lib/python3.11/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain) (2.33.2)\n",
            "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain) (0.4.1)\n",
            "Collecting python-dotenv>=0.21.0 (from pydantic-settings<3.0.0,>=2.4.0->langchain-community)\n",
            "  Downloading python_dotenv-1.1.1-py3-none-any.whl.metadata (24 kB)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2->langchain) (3.4.2)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2->langchain) (2.4.0)\n",
            "Requirement already satisfied: greenlet>=1 in /usr/local/lib/python3.11/dist-packages (from SQLAlchemy<3,>=1.4->langchain) (3.2.3)\n",
            "Requirement already satisfied: jsonpointer>=1.9 in /usr/local/lib/python3.11/dist-packages (from jsonpatch<2.0,>=1.33->langchain-core<1.0.0,>=0.3.66->langchain) (3.0.0)\n",
            "Collecting mypy-extensions>=0.3.0 (from typing-inspect<1,>=0.4.0->dataclasses-json<0.7,>=0.5.7->langchain-community)\n",
            "  Downloading mypy_extensions-1.1.0-py3-none-any.whl.metadata (1.1 kB)\n",
            "Downloading pymupdf-1.26.3-cp39-abi3-manylinux_2_28_x86_64.whl (24.1 MB)\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m24.1/24.1 MB\u001b[0m \u001b[31m57.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading faiss_cpu-1.11.0-cp311-cp311-manylinux_2_28_x86_64.whl (31.3 MB)\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m31.3/31.3 MB\u001b[0m \u001b[31m13.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading langchain_community-0.3.27-py3-none-any.whl (2.5 MB)\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m2.5/2.5 MB\u001b[0m \u001b[31m64.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading dataclasses_json-0.6.7-py3-none-any.whl (28 kB)\n",
            "Downloading httpx_sse-0.4.1-py3-none-any.whl (8.1 kB)\n",
            "Downloading pydantic_settings-2.10.1-py3-none-any.whl (45 kB)\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m45.2/45.2 kB\u001b[0m \u001b[31m3.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading marshmallow-3.26.1-py3-none-any.whl (50 kB)\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m50.9/50.9 kB\u001b[0m \u001b[31m4.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading python_dotenv-1.1.1-py3-none-any.whl (20 kB)\n",
            "Downloading typing_inspect-0.9.0-py3-none-any.whl (8.8 kB)\n",
            "Downloading mypy_extensions-1.1.0-py3-none-any.whl (5.0 kB)\n",
            "Installing collected packages: python-dotenv, pymupdf, mypy-extensions, marshmallow, httpx-sse, faiss-cpu, typing-inspect, pydantic-settings, dataclasses-json, langchain-community\n",
            "Successfully installed dataclasses-json-0.6.7 faiss-cpu-1.11.0 httpx-sse-0.4.1 langchain-community-0.3.27 marshmallow-3.26.1 mypy-extensions-1.1.0 pydantic-settings-2.10.1 pymupdf-1.26.3 python-dotenv-1.1.1 typing-inspect-0.9.0\n"
          ]
        }
      ],
      "source": [
        "pip install pymupdf langchain faiss-cpu openai tiktoken langchain-community\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import fitz  # PyMuPDF\n",
        "from PIL import Image\n",
        "import numpy as np\n",
        "from io import BytesIO\n",
        "from tqdm import tqdm\n",
        "from langchain.embeddings import OpenAIEmbeddings as OE\n",
        "from langchain.vectorstores import FAISS\n",
        "from sentence_transformers import SentenceTransformer\n",
        "from langchain.chat_models import ChatOpenAI\n",
        "from langchain.chains import RetrievalQA\n",
        "\n",
        "from transformers import BlipProcessor, BlipForConditionalGeneration\n",
        "import torch\n",
        "\n",
        "# Load BLIP for image captioning\n",
        "processor = BlipProcessor.from_pretrained(\"Salesforce/blip-image-captioning-base\")\n",
        "blip_model = BlipForConditionalGeneration.from_pretrained(\"Salesforce/blip-image-captioning-base\").eval()\n",
        "\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "blip_model.to(device)\n",
        "\n",
        "# API Key\n",
        "os.environ[\"OPENAI_API_KEY\"] = os.getenv(\"OPENAI_API_KEY\", \"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\")\n",
        "\n",
        "\n",
        "# Embedding models\n",
        "text_emb = OE(model=\"text-embedding-3-small\")\n",
        "image_emb = SentenceTransformer(\"clip-ViT-B-32\")\n",
        "\n",
        "def describe_image(image_path):\n",
        "    raw_image = Image.open(image_path).convert('RGB')\n",
        "    inputs = processor(raw_image, return_tensors=\"pt\").to(device)\n",
        "    out = blip_model.generate(**inputs)\n",
        "    caption = processor.decode(out[0], skip_special_tokens=True)\n",
        "    return caption\n",
        "\n",
        "def extract_pdf_pages(pdf_path, image_dir=\"imgs\"):\n",
        "    os.makedirs(image_dir, exist_ok=True)\n",
        "    doc = fitz.open(pdf_path)\n",
        "    pages = []\n",
        "\n",
        "    for i, page in enumerate(doc, 1):\n",
        "        txt = page.get_text()\n",
        "        imgs = []\n",
        "\n",
        "        for img in page.get_images(full=True):\n",
        "            xref = img[0]\n",
        "            try:\n",
        "                pix = fitz.Pixmap(doc, xref)\n",
        "                if pix.colorspace.n not in [1, 3]:  # Not grayscale or RGB\n",
        "                    pix = fitz.Pixmap(fitz.csRGB, pix)\n",
        "                buf = pix.tobytes(\"png\")\n",
        "                path = f\"{image_dir}/pg{i}_img{xref}.png\"\n",
        "                Image.open(BytesIO(buf)).save(path)\n",
        "                imgs.append(path)\n",
        "            except Exception as e:\n",
        "                #print(f\"Skipping image {xref} on page {i}: {e}\")\n",
        "                print(\".\")\n",
        "\n",
        "        pages.append({\"page\": i, \"text\": txt, \"images\": imgs})\n",
        "    return pages\n",
        "\n",
        "def build_embeddings(pages, text_batch_size=50):\n",
        "    texts, metadatas = [], []\n",
        "\n",
        "    print(\"Processing pages...\")\n",
        "    for p in pages:\n",
        "        if p[\"text\"].strip():\n",
        "            texts.append(p[\"text\"])\n",
        "            metadatas.append({\"type\": \"text\", \"page\": p[\"page\"]})\n",
        "\n",
        "        for im in p[\"images\"]:\n",
        "            caption = describe_image(im)\n",
        "            texts.append(caption)\n",
        "            metadatas.append({\n",
        "                \"type\": \"image_caption\",\n",
        "                \"page\": p[\"page\"],\n",
        "                \"path\": im,\n",
        "                \"caption\": caption\n",
        "            })\n",
        "\n",
        "    text_vecs = []\n",
        "    for i in tqdm(range(0, len(texts), text_batch_size), desc=\"Embedding text and captions\"):\n",
        "        batch = texts[i : i + text_batch_size]\n",
        "        text_vecs.extend(text_emb.embed_documents(batch))\n",
        "    text_vecs = np.array(text_vecs)\n",
        "\n",
        "    return text_vecs, texts, metadatas\n",
        "\n",
        "def create_vectorstore(pages, index_path=\"mm_faiss\"):\n",
        "    idx, docs, metas = build_embeddings(pages)\n",
        "    text_embedding_pairs = list(zip(docs, idx))\n",
        "\n",
        "    vs = FAISS.from_embeddings(\n",
        "        text_embedding_pairs,\n",
        "        embedding=text_emb,\n",
        "        metadatas=metas\n",
        "    )\n",
        "    vs.save_local(index_path)\n",
        "    return vs\n",
        "\n",
        "def load_or_create(pdf_path):\n",
        "    if os.path.exists(\"mm_faiss/index.faiss\"):\n",
        "        vs = FAISS.load_local(\"mm_faiss\", text_emb, allow_dangerous_deserialization=True)\n",
        "    else:\n",
        "        pages = extract_pdf_pages(pdf_path)\n",
        "        vs = create_vectorstore(pages)\n",
        "    return vs\n",
        "\n",
        "def hybrid_rag_chain(vs, k=5):\n",
        "    retriever = vs.as_retriever(search_kwargs={\"k\": k})\n",
        "    llm = ChatOpenAI(model_name=\"gpt-4.1-mini\", temperature=0.2)\n",
        "    return RetrievalQA.from_chain_type(llm=llm, retriever=retriever, return_source_documents=True)\n",
        "\n",
        "def query_rag(rag, user_q):\n",
        "    result = rag({\"query\": user_q})\n",
        "    return result[\"result\"], result[\"source_documents\"]\n",
        "\n",
        "# MAIN\n",
        "if __name__ == \"__main__\":\n",
        "    path = \"/content/drive/MyDrive/AI Summary Trial Run/3-8-24 combined missing 2  or more maybe-sm.pdf\"  # Replace with your actual PDF path\n",
        "    query = \"What are the key takeaways from the document?\"\n",
        "\n",
        "    vs = load_or_create(path)\n",
        "    rag = hybrid_rag_chain(vs, k=5)\n",
        "\n",
        "    answer, docs = query_rag(rag, query)\n",
        "\n",
        "    print(\"\\nüß† Answer:\\n\", answer)\n",
        "    print(\"\\nüìö Retrieved Sources:\")\n",
        "    for d in docs:\n",
        "        print(d.metadata, d.page_content[:100], \"...\" if len(d.page_content) > 100 else \"\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "w40F14DZzzuA",
        "outputId": "c45b76e3-acfc-4277-9cc2-a6a413934661"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚ùå Skipping image 3458 on page 505: code=4: pixmap must be grayscale or rgb to write as png\n",
            "‚ùå Skipping image 1364 on page 623: code=4: pixmap must be grayscale or rgb to write as png\n",
            "‚ùå Skipping image 1518 on page 686: 'NoneType' object has no attribute 'n'\n",
            "‚ùå Skipping image 1519 on page 686: 'NoneType' object has no attribute 'n'\n",
            "‚ùå Skipping image 3458 on page 705: code=4: pixmap must be grayscale or rgb to write as png\n",
            "‚ùå Skipping image 1659 on page 745: 'NoneType' object has no attribute 'n'\n",
            "‚ùå Skipping image 1660 on page 745: 'NoneType' object has no attribute 'n'\n",
            "‚ùå Skipping image 1665 on page 746: 'NoneType' object has no attribute 'n'\n",
            "‚ùå Skipping image 1666 on page 746: 'NoneType' object has no attribute 'n'\n",
            "‚ùå Skipping image 1667 on page 746: 'NoneType' object has no attribute 'n'\n",
            "‚ùå Skipping image 1672 on page 747: 'NoneType' object has no attribute 'n'\n",
            "‚ùå Skipping image 1673 on page 747: 'NoneType' object has no attribute 'n'\n",
            "‚ùå Skipping image 1674 on page 747: 'NoneType' object has no attribute 'n'\n",
            "‚ùå Skipping image 1675 on page 747: 'NoneType' object has no attribute 'n'\n",
            "‚ùå Skipping image 1895 on page 841: code=4: pixmap must be grayscale or rgb to write as png\n",
            "‚ùå Skipping image 2040 on page 902: code=4: pixmap must be grayscale or rgb to write as png\n",
            "Processing pages...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Embedding text and captions: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 30/30 [00:42<00:00,  1.43s/it]\n",
            "/tmp/ipython-input-6-2406858809.py:112: LangChainDeprecationWarning: The class `ChatOpenAI` was deprecated in LangChain 0.0.10 and will be removed in 1.0. An updated version of the class exists in the :class:`~langchain-openai package and should be used instead. To use it run `pip install -U :class:`~langchain-openai` and import as `from :class:`~langchain_openai import ChatOpenAI``.\n",
            "  llm = ChatOpenAI(model_name=\"gpt-4.1-mini\", temperature=0.2)\n",
            "/tmp/ipython-input-6-2406858809.py:116: LangChainDeprecationWarning: The method `Chain.__call__` was deprecated in langchain 0.1.0 and will be removed in 1.0. Use :meth:`~invoke` instead.\n",
            "  result = rag({\"query\": user_q})\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "üß† Answer:\n",
            " I‚Äôm happy to help! Could you please specify which document you are referring to?\n",
            "\n",
            "üìö Retrieved Sources:\n",
            "{'type': 'image_caption', 'page': 105, 'path': 'imgs/pg105_img3368.png', 'caption': 'a document with the text and description of the document'} a document with the text and description of the document \n",
            "{'type': 'image_caption', 'page': 173, 'path': 'imgs/pg173_img3349.png', 'caption': 'a document with the text and description of the document'} a document with the text and description of the document \n",
            "{'type': 'image_caption', 'page': 175, 'path': 'imgs/pg175_img3351.png', 'caption': 'a document with the text and description of the document'} a document with the text and description of the document \n",
            "{'type': 'image_caption', 'page': 192, 'path': 'imgs/pg192_img3368.png', 'caption': 'a document with the text and description of the document'} a document with the text and description of the document \n",
            "{'type': 'image_caption', 'page': 86, 'path': 'imgs/pg86_img3349.png', 'caption': 'a document with the text and description of the document'} a document with the text and description of the document \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# MAIN\n",
        "if __name__ == \"__main__\":\n",
        "    path = \"/content/drive/MyDrive/AI Summary Trial Run/3-8-24 combined missing 2  or more maybe-sm.pdf\"  # Replace with your actual PDF path\n",
        "\n",
        "    query = \"What enforcement actions can the Control Authority take if an industrial user continues to violate wastewater discharge requirements?\"\n",
        "\n",
        "    vs = load_or_create(path)\n",
        "    rag = hybrid_rag_chain(vs, k=5)\n",
        "\n",
        "    answer, docs = query_rag(rag, query)\n",
        "\n",
        "    print(\"\\nüß† Answer:\\n\", answer)\n",
        "    print(\"\\nüìö Retrieved Sources:\")\n",
        "    for d in docs:\n",
        "        print(d.metadata, d.page_content[:100], \"...\" if len(d.page_content) > 100 else \"\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "v9BYDdJ523J6",
        "outputId": "17f51b9d-dab5-4efa-a4d4-23cc7cac3f6f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "üß† Answer:\n",
            " If an industrial user continues to violate wastewater discharge requirements, the Control Authority can take several enforcement actions based on the Enforcement Response Plan (ERP) it has developed. These actions include:\n",
            "\n",
            "(A) Notification of Violation: Serving a notice of the violation to the user, requiring a plan for correction and prevention.\n",
            "\n",
            "(B) Consent Orders: Entering into agreements with the user to correct the noncompliance within a specified time.\n",
            "\n",
            "(C) Show Cause Hearing: Ordering the user to show cause why enforcement action should not be taken, with notice given at least 10 days prior.\n",
            "\n",
            "(D) Compliance Orders: Issuing orders that may direct disconnection of sewer service after a specified time unless adequate treatment is installed and operated; may also require pretreatment technology, additional monitoring, and management practices.\n",
            "\n",
            "(E) Cease and Desist Orders: Ordering the user to immediately comply and take remedial or preventative actions, including halting operations and terminating discharge if necessary.\n",
            "\n",
            "(F) Administrative Fines: Imposing fines up to $2,500 per violation, with each day of continued noncompliance considered a separate violation; fines may be added to the user's sewer service charge and collected accordingly.\n",
            "\n",
            "These enforcement measures allow the Control Authority to address ongoing violations effectively and ensure compliance with wastewater discharge regulations.\n",
            "\n",
            "üìö Retrieved Sources:\n",
            "{'type': 'text', 'page': 349} 32 \n",
            "13.20.400 Enforcement system. \n",
            "Industries found to be in violation of any pretreatment requireme ...\n",
            "{'type': 'text', 'page': 940} 32 \n",
            "13.20.400 Enforcement system. \n",
            "Industries found to be in violation of any pretreatment requireme ...\n",
            "{'type': 'text', 'page': 1050} Industries found to be in violation of any pretreatment requirements will receive notice of such\n",
            "ins ...\n",
            "{'type': 'text', 'page': 459} Industries found to be in violation of any pretreatment requirements will receive notice of such\n",
            "ins ...\n",
            "{'type': 'text', 'page': 1309} Industries found to be in violation of any pretreatment requirements will receive notice of such\n",
            "ins ...\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# MAIN\n",
        "if __name__ == \"__main__\":\n",
        "    path = \"/content/drive/MyDrive/AI Summary Trial Run/3-8-24 combined missing 2  or more maybe-sm.pdf\"  # Replace with your actual PDF path\n",
        "\n",
        "    query = \"What type of encroachment was applied for by McCurdy Development, LLC in the permit submitted on 11-22-17?\"\n",
        "\n",
        "    vs = load_or_create(path)\n",
        "    rag = hybrid_rag_chain(vs, k=5)\n",
        "\n",
        "    answer, docs = query_rag(rag, query)\n",
        "\n",
        "    print(\"\\nüß† Answer:\\n\", answer)\n",
        "    print(\"\\nüìö Retrieved Sources:\")\n",
        "    for d in docs:\n",
        "        print(d.metadata, d.page_content[:100], \"...\" if len(d.page_content) > 100 else \"\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HnuWrwTk7hpo",
        "outputId": "3b943a7c-ee7f-43d3-b017-6d88185bc91a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "üß† Answer:\n",
            " The encroachment permit application submitted by McCurdy Development, LLC on 11-22-17 was for encroachments in the public right-of-way or drainage easements, specifically at the location South Bound Riverside Drive with coordinates approximately 37.968154 -87.574564. The application indicates work involving crossing perpendicular to the street centerline and encroaching on the right-of-way line.\n",
            "\n",
            "üìö Retrieved Sources:\n",
            "{'type': 'text', 'page': 521} City of Evansville\n",
            "APPLICATION FOR ENCROACHMENT PERMIT\n",
            "(Permit for the purpose of Encroachments in P ...\n",
            "{'type': 'text', 'page': 720} City of Evansville\n",
            "APPLICATION FOR ENCROACHMENT PERMIT\n",
            "(Permit for the purpose of Encroachments in P ...\n",
            "{'type': 'text', 'page': 535}  \n",
            " \n",
            " \n",
            "McCurdy 100 Development, LLC \n",
            "October 2, 2017 \n",
            "Page 3 \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            "Enclosure \n",
            " \n",
            "cc: \n",
            "Evansville  ...\n",
            "{'type': 'text', 'page': 861} ‚Äúfabricated‚Äù, ‚Äúreckless‚Äù and ‚Äúactionable‚Äù. EWSU would direct Development‚Äôs\n",
            "attention to EMC 13.05.14 ...\n",
            "{'type': 'text', 'page': 561} From: \"Merrick, Jeff\" <JMerrick@ewsu.com>\n",
            "To: 'Marco DeLucio' <mdelucio@zsws.com>\n",
            "Cc: \"Claspell, Dan ...\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# MAIN\n",
        "if __name__ == \"__main__\":\n",
        "    path = \"/content/drive/MyDrive/AI Summary Trial Run/3-8-24 combined missing 2  or more maybe-sm.pdf\"  # Replace with your actual PDF path\n",
        "\n",
        "    query = \"Who signed the affidavit for the Evansville Sewage Works Department on July 10, 2018, and what was the purpose of the document?\"\n",
        "\n",
        "    vs = load_or_create(path)\n",
        "    rag = hybrid_rag_chain(vs, k=5)\n",
        "\n",
        "    answer, docs = query_rag(rag, query)\n",
        "\n",
        "    print(\"\\nüß† Answer:\\n\", answer)\n",
        "    print(\"\\nüìö Retrieved Sources:\")\n",
        "    for d in docs:\n",
        "        print(d.metadata, d.page_content[:100], \"...\" if len(d.page_content) > 100 else \"\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4wcA8AS98obJ",
        "outputId": "57afc757-39d8-43ec-8042-856c4613728a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "üß† Answer:\n",
            " The affidavit for the Evansville Sewage Works Department on July 10, 2018, was signed by Sarah Burlison, Accounts Receivable Clerk. The purpose of the document was to affirm, under penalties of perjury, that reasonable care was taken to redact each Social Security number in the document unless required by law. The affidavit was notarized by Ashley Mosby, a Notary Public in Vanderburgh County, Indiana.\n",
            "\n",
            "üìö Retrieved Sources:\n",
            "{'type': 'text', 'page': 1326} 82C01-1807-PL-004109\n",
            "Vanderburgh Circuit Court\n",
            "EXHIBIT B\n",
            "Filed: 7/25/2018 12:38 PM\n",
            "Clerk\n",
            "Vanderburgh ...\n",
            "{'type': 'text', 'page': 1403} 82C01-1807-PL-004109\n",
            "Vanderburgh Circuit Court\n",
            "EXHIBIT B\n",
            "Filed: 7/25/2018 12:38 PM\n",
            "Clerk\n",
            "Vanderburgh ...\n",
            "{'type': 'text', 'page': 1430} 82C01-1807-PL-004109\n",
            "Vanderburgh Circuit Court\n",
            "EXHIBIT B\n",
            "Filed: 7/25/2018 12:38 PM\n",
            "Clerk\n",
            "Vanderburgh ...\n",
            "{'type': 'text', 'page': 1407} 82C01-1807-PL-004109\n",
            "Vanderburgh Circuit Court\n",
            "EXHIBIT C\n",
            "Filed: 7/25/2018 12:38 PM\n",
            "Clerk\n",
            "Vanderburgh ...\n",
            "{'type': 'text', 'page': 1434} 82C01-1807-PL-004109\n",
            "Vanderburgh Circuit Court\n",
            "EXHIBIT C\n",
            "Filed: 7/25/2018 12:38 PM\n",
            "Clerk\n",
            "Vanderburgh ...\n"
          ]
        }
      ]
    }
  ]
}